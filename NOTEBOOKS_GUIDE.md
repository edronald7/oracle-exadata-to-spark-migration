# ğŸ“š GuÃ­a de Notebooks Jupyter

**Ãšltima actualizaciÃ³n**: 2026-01-12  
**Total de notebooks**: 3

---

## ğŸ¯ Resumen

Los notebooks Jupyter proporcionan experiencias **hands-on** ejecutables para aprender los casos mÃ¡s crÃ­ticos del repositorio.

Cada notebook incluye:
- âœ… CÃ³digo ejecutable paso a paso
- âœ… Datos sintÃ©ticos generados automÃ¡ticamente
- âœ… Explicaciones detalladas
- âœ… Validaciones y verificaciones
- âœ… Conclusiones y best practices

---

## ğŸ““ Notebooks Disponibles

### 1. Getting Started (01-getting-started.ipynb)

**Status**: âœ… Iniciado  
**Nivel**: Principiante  
**DuraciÃ³n**: 15-20 minutos

**Contenido**:
- Setup inicial de Spark
- Primeros queries SQL vs PySpark
- Comparativa bÃ¡sica de sintaxis

**CÃ³mo ejecutar**:
```bash
cd notebooks
jupyter notebook 01-getting-started.ipynb
```

---

### 2. CDC e Ingesta Incremental (13-cdc-incremental.ipynb)

**Status**: âœ… **COMPLETO**  
**Nivel**: Intermedio-Avanzado  
**DuraciÃ³n**: 30-45 minutos  
**Criticidad**: â­â­â­â­â­ (MÃXIMA)

**Lo que aprenderÃ¡s**:
1. Generar datos CDC sintÃ©ticos (simula Oracle GoldenGate)
2. Implementar deduplicaciÃ³n con Window Functions
3. Crear tabla Delta Lake
4. Ejecutar MERGE operation (UPDATE/DELETE/INSERT)
5. Validar resultados
6. Usar Time Travel para auditorÃ­a

**Celdas incluidas**: 17 celdas
- 8 celdas de cÃ³digo ejecutable
- 9 celdas markdown con explicaciones

**Highlights**:
```python
# DeduplicaciÃ³n
window_spec = Window.partitionBy("customer_id").orderBy(col("op_ts").desc())
deduplicated = cdc_df.withColumn("rn", row_number().over(window_spec))

# MERGE a Delta Lake
target_table.merge(source, condition)
  .whenMatchedUpdate(...)
  .whenMatchedDelete(...)
  .whenNotMatchedInsert(...)
```

**CÃ³mo ejecutar**:
```bash
cd notebooks
jupyter notebook 13-cdc-incremental.ipynb
```

**Prerrequisitos**:
- Spark 3.x
- Delta Lake (`pip install delta-spark`)

---

### 3. Data Quality y Error Handling (14-data-quality.ipynb)

**Status**: âœ… **COMPLETO**  
**Nivel**: Intermedio  
**DuraciÃ³n**: 30 minutos  
**Criticidad**: â­â­â­â­â­ (MÃXIMA)

**Lo que aprenderÃ¡s**:
1. Generar datos con errores (JSON corruptos, nulls, valores fuera de rango)
2. Leer con PERMISSIVE mode para capturar bad records
3. Aplicar validaciones de negocio
4. Separar datos vÃ¡lidos e invÃ¡lidos
5. Implementar quarantine tables
6. Generar reportes de data quality

**Celdas incluidas**: 17 celdas
- 8 celdas de cÃ³digo ejecutable
- 9 celdas markdown con explicaciones

**Problemas que detecta**:
- âŒ JSON malformado (30 registros)
- âŒ Emails sin @ (30 registros)
- âŒ Edad negativa (20 registros)
- âŒ IDs nulos (20 registros)

**Highlights**:
```python
# PERMISSIVE mode
raw_df = spark.read \
    .option("mode", "PERMISSIVE") \
    .option("columnNameOfCorruptRecord", "_corrupt_record") \
    .json("file.json")

# Validaciones
validated = df.withColumn("quality_status",
    when(col("id").isNull(), "NULL_ID")
    .when(~col("email").contains("@"), "INVALID_EMAIL")
    .otherwise("VALID")
)

# Quarantine
invalid_records.write.partitionBy("reason").parquet("/quarantine")
```

**CÃ³mo ejecutar**:
```bash
cd notebooks
jupyter notebook 14-data-quality.ipynb
```

---

## ğŸš€ Quick Start

### OpciÃ³n 1: Localmente con Docker

```bash
# 1. Clonar repo
git clone https://github.com/tu-usuario/oracle-exadata-to-spark-migration.git
cd oracle-exadata-to-spark-migration

# 2. Iniciar Jupyter con Spark
docker run -it -p 8888:8888 \
  -v $(pwd):/workspace \
  jupyter/pyspark-notebook

# 3. Abrir notebooks en http://localhost:8888
```

### OpciÃ³n 2: Azure Databricks

```bash
# 1. Crear workspace en Azure Portal
# 2. En Databricks UI:
#    - Workspace â†’ Import
#    - Seleccionar notebooks/*.ipynb
#    - Ejecutar interactivamente
```

### OpciÃ³n 3: AWS EMR con JupyterHub

```bash
# Ver cloud/aws/emr-setup.md para instrucciones detalladas

# 1. Crear cluster con Jupyter
aws emr create-cluster \
  --applications Name=Spark Name=JupyterHub \
  --release-label emr-7.0.0 \
  ...

# 2. Acceder a JupyterHub en puerto 9443
# 3. Upload notebooks
```

---

## ğŸ“Š Estado de Notebooks

| Notebook | Status | Celdas | Nivel | Tiempo |
|----------|--------|--------|-------|--------|
| 01-getting-started | âœ… Iniciado | 10+ | Principiante | 15min |
| 13-cdc-incremental | âœ… **COMPLETO** | 17 | Avanzado | 45min |
| 14-data-quality | âœ… **COMPLETO** | 17 | Intermedio | 30min |
| 15-streaming | ğŸ”œ Pendiente | â€” | Avanzado | â€” |
| 17-cost-optimization | ğŸ”œ Pendiente | â€” | Intermedio | â€” |

---

## ğŸ“ Ruta de Aprendizaje Recomendada

### Para Analistas SQL
1. âœ… **01-getting-started** - IntroducciÃ³n
2. âœ… **13-cdc-incremental** - Ver queries SQL
3. Leer READMEs de casos 02-12

### Para Data Engineers
1. âœ… **01-getting-started** - Setup
2. âœ… **14-data-quality** - Manejo de errores (CRÃTICO)
3. âœ… **13-cdc-incremental** - CDC patterns (CRÃTICO)
4. ğŸ”œ **15-streaming** - Real-time (prÃ³ximamente)
5. Implementar casos en producciÃ³n

### Para Arquitectos
1. Revisar todos los notebooks para entender implementaciones
2. Leer `docs/cloud-deployment-guide.md`
3. DiseÃ±ar arquitectura usando patrones aprendidos

---

## ğŸ’¡ Tips para Usar los Notebooks

### EjecuciÃ³n Secuencial
- âš ï¸ **Ejecuta las celdas en orden** (no saltar pasos)
- Algunas celdas dependen de resultados anteriores

### Limpieza de Datos
```python
# Si necesitas reiniciar:
import shutil
shutil.rmtree("/tmp/cdc_data", ignore_errors=True)
shutil.rmtree("/tmp/delta", ignore_errors=True)
# Luego re-ejecutar desde el inicio
```

### Modificar Volumen de Datos
```python
# En generadores, cambiar:
spark.range(1, 1001)  # 1000 registros
# A:
spark.range(1, 10001)  # 10,000 registros
```

### Verificar Spark UI
```bash
# Mientras el notebook estÃ¡ corriendo:
# Abrir http://localhost:4040 para ver Spark UI
# Ver stages, tasks, DAG, storage
```

---

## ğŸ”§ Troubleshooting

### Error: Delta Lake no encontrado
```bash
pip install delta-spark==2.4.0
```

### Error: OutOfMemoryError
```python
# Aumentar memoria del driver
spark = SparkSession.builder \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()
```

### Error: Permission denied en /tmp
```python
# Cambiar rutas a directorio del usuario
output_path = "/Users/tu-usuario/spark-data/cdc"
```

---

## ğŸ“š Recursos Adicionales

### DocumentaciÃ³n Complementaria
- `cases/13-cdc-incremental/README.md` - TeorÃ­a completa
- `cases/14-data-quality/README.md` - Patrones avanzados
- `docs/pyspark-best-practices.md` - Best practices

### CÃ³digo de Referencia
- `cases/13-cdc-incremental/3_pyspark.py` - CDC production
- `templates/pyspark/validation.py` - Validations framework

---

## ğŸ”œ PrÃ³ximos Notebooks Planeados

1. **15-streaming.ipynb** (Prioridad Alta)
   - Kafka â†’ Spark â†’ Delta Lake
   - Watermarking
   - Windowed aggregations

2. **17-cost-optimization.ipynb** (Prioridad Alta)
   - Analizar Spark UI
   - Optimizar queries
   - Reducir costos 10x

3. **02-smart-scan.ipynb** (Prioridad Media)
   - Predicate pushdown
   - Partition pruning
   - Column pruning

---

## ğŸ¤ Contribuir

Â¿Quieres agregar un notebook?

1. Usa notebooks existentes como template
2. Incluye:
   - Markdown intro con objetivos
   - Setup cell
   - CÃ³digo ejecutable paso a paso
   - Validaciones
   - Conclusiones
3. Testea completamente
4. Abre PR

---

## ğŸ“ Soporte

- **Issues**: Reportar problemas con notebooks
- **Discussions**: Preguntas sobre ejecuciÃ³n
- **Email**: edronald7@gmail.com

---

**Ãšltima actualizaciÃ³n**: 2026-01-12  
**Mantenido por**: edronald7@gmail.com + community  
**Licencia**: MIT
