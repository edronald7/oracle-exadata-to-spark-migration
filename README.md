# Oracle Exadata ‚Üí Apache Spark: Gu√≠a Comparativa SQL y Program√°tica

**Repositorio de aprendizaje** para analistas y data engineers que trabajan con Oracle Exadata y necesitan aprender el ecosistema Apache Spark en la nube.

> **üéØ Objetivo**: Comparar sintaxis, patrones y mejores pr√°cticas entre:
> - **Oracle Exadata SQL** (Smart Scan, Storage Offload, HCC, Indexes)
> - **Spark SQL** (data layout, partitioning, bucketing, file formats)
> - **PySpark** (DataFrame API, Python)
> - **Spark Scala** (Dataset API, type-safe)

---

## üë• ¬øPara qui√©n es este repositorio?

| Perfil | Qu√© encontrar√°s |
|--------|-----------------|
| **Analistas SQL** | Traducci√≥n directa de queries Oracle a Spark SQL con explicaciones |
| **Data Engineers** | Implementaciones en PySpark y Scala con best practices |
| **Arquitectos de Datos** | Estrategias de migraci√≥n y dise√±o de data lakes |
| **DBAs Oracle** | Mapeo de features Exadata a capacidades de Spark |
| **Equipos Cloud** | Gu√≠as de deployment en AWS, Azure y GCP |

---

## üéì Rutas de Aprendizaje

### üìò Ruta para Analistas (SQL-first)
1. Lee [SQL vs DataFrame API](docs/sql-vs-dataframe-api.md)
2. Comienza con [Caso 02: Smart Scan](cases/02-smart-scan-filter-pushdown/)
3. Practica con notebooks en `notebooks/`
4. Ejecuta en [Databricks](cloud/azure/databricks-setup.md) o [EMR](cloud/aws/emr-setup.md)

### üíª Ruta para Engineers (Code-first)
1. Lee [PySpark Best Practices](docs/pyspark-best-practices.md)
2. Estudia implementaciones en `cases/*/3_pyspark.py` y `cases/*/4_scala.scala`
3. Ejecuta generadores de datos: `data/generators/`
4. Implementa pipelines siguiendo [runbooks](runbooks/)

### üèóÔ∏è Ruta para Arquitectos (Strategy-first)
1. Lee [Mapa de features Exadata ‚Üí Spark](docs/exadata-feature-map.md)
2. Revisa [Checklist de migraci√≥n](docs/migration-checklist.md)
3. Dise√±a arquitectura con [Cloud Deployment Guide](docs/cloud-deployment-guide.md)
4. Valida con [estrategia de testing](docs/validation-strategy.md)

---

## üèóÔ∏è ¬øQu√© incluye?

- **`docs/`** ‚Äî Gu√≠as conceptuales y estrat√©gicas
  - Mapeo de features Exadata ‚Üí Spark
  - Best practices PySpark y Scala
  - Estrategias de deployment en cloud
- **`cases/`** ‚Äî 12 casos de uso con **4 implementaciones** cada uno:
  - `1_oracle.sql` - Query Oracle original
  - `2_sparksql.sql` - Equivalente en Spark SQL
  - `3_pyspark.py` - Implementaci√≥n PySpark
  - `4_scala.scala` - Implementaci√≥n Scala
- **`data/`** ‚Äî Generadores de datos de prueba ejecutables
- **`notebooks/`** ‚Äî Jupyter y Databricks notebooks interactivos
- **`cloud/`** ‚Äî Gu√≠as espec√≠ficas para AWS, Azure y GCP
- **`templates/`** ‚Äî C√≥digo reutilizable (validaci√≥n, performance, testing)
- **`runbooks/`** ‚Äî Procedimientos paso a paso de deployment
- **`snippets/`** ‚Äî Fragmentos de c√≥digo reutilizables

---

## üìö √çndice de Casos

Cada caso incluye: **Oracle SQL** | **Spark SQL** | **PySpark** | **Scala** | **Datos de prueba** | **Notebook**

### üîµ Casos B√°sicos: Features Exadata Core

| # | Caso | Exadata Feature | SQL | PySpark | Scala | Notebook |
|---|------|-----------------|-----|---------|-------|----------|
| 01 | [Hints & Parallel](cases/01-hints-parallel/) | `/*+ PARALLEL */`, optimizer hints | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| 02 | [Smart Scan / Filter Pushdown](cases/02-smart-scan-filter-pushdown/) | Storage offload, predicate pushdown | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| 03 | [Partition Pruning](cases/03-partition-pruning/) | Range/list partitions, partition-wise ops | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| 04 | [Indexes vs File Layout](cases/04-indexes-vs-layout/) | B-tree/bitmap indexes | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| 05 | [Star Joins / Bloom Filters](cases/05-star-joins-bloom/) | Bloom filter join acceleration | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| 06 | [Materialized Views](cases/06-materialized-views/) | MV query rewrite, refresh | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| 07 | [Result Cache](cases/07-result-cache/) | Result cache, query cache | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| 08 | [Flashback / Time Travel](cases/08-flashback-time-travel/) | `AS OF TIMESTAMP`, flashback | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| 09 | [Window Analytics](cases/09-window-analytics/) | Window functions, analytics | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| 10 | [MERGE / SCD](cases/10-merge-scd/) | MERGE statement, upserts | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| 11 | [Datatypes & NLS](cases/11-datatypes-nls/) | NUMBER, DATE, NLS settings | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| 12 | [Set Semantics](cases/12-set-semantics/) | MINUS, INTERSECT, duplicates | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |

### ‚≠ê Casos Avanzados: D√≠a a D√≠a del Data Engineer

| # | Caso | Tema | Criticidad | SQL | PySpark | Scala | Notebook |
|---|------|------|------------|-----|---------|-------|----------|
| 13 | [CDC / Incremental Ingestion](cases/13-cdc-incremental/) | Change Data Capture, MERGE, dedup | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚úÖ | ‚úÖ | üîú |
| 14 | [Data Quality & Error Handling](cases/14-data-quality/) | Bad records, quarantine, Great Expectations | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚úÖ | üîú | üîú |
| 15 | [Spark Structured Streaming](cases/15-streaming/) | Kafka, near real-time, watermarking | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚úÖ | üîú | üîú |
| 16 | [Orquestaci√≥n Airflow](cases/16-orchestration/) | DAGs, scheduling, retries | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚Äî | ‚úÖ | üîú | üîú |
| 17 | [Cost Optimization](cases/17-cost-optimization/) | Shuffle, compaction, spot instances | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚úÖ | üîú | üîú |
| 18 | [Schema Evolution](cases/18-schema-evolution/) | ADD/DROP columns, mergeSchema | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚úÖ | üîú | üîú |
| 19 | [Troubleshooting & Debugging](cases/19-troubleshooting/) | OOM, skew, slow queries, Spark UI | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚Äî | ‚úÖ | üîú | üîú |
| 20 | [Integraciones Ecosistema](cases/20-integrations/) | JDBC, Kafka, Redshift, Snowflake, MLflow | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚úÖ | üîú | üîú |

> üí° **Leyenda**: ‚úÖ Disponible | üîú Pr√≥ximamente | ‚Äî No aplica

---

## üöÄ Quick Start

### Opci√≥n 1: Ejecutar Localmente (Docker)

```bash
# 1. Clonar repositorio
git clone https://github.com/tu-usuario/oracle-exadata-to-spark-migration.git
cd oracle-exadata-to-spark-migration

# 2. Iniciar Spark con Docker
docker run -it -p 8888:8888 \
  -v $(pwd):/workspace \
  jupyter/pyspark-notebook

# 3. Generar datos de prueba
python data/generators/generate_all.py --size small

# 4. Ejecutar caso de ejemplo
cd cases/01-hints-parallel
spark-submit 3_pyspark.py
```

### Opci√≥n 2: Cloud (AWS EMR)

```bash
# Ver gu√≠a completa en cloud/aws/emr-setup.md
aws emr create-cluster --name "Spark-Learning" \
  --release-label emr-7.0.0 \
  --applications Name=Spark Name=Jupyter
```

### Opci√≥n 3: Cloud (Azure Databricks)

```bash
# Ver gu√≠a completa en cloud/azure/databricks-setup.md
# 1. Crear workspace en Azure Portal
# 2. Importar notebooks desde notebooks/databricks/
# 3. Ejecutar interactivamente
```

### Opci√≥n 4: Cloud (GCP Dataproc)

```bash
# Ver gu√≠a completa en cloud/gcp/dataproc-setup.md
gcloud dataproc clusters create spark-learning \
  --region=us-central1 \
  --image-version=2.1
```

---

## üìñ Documentaci√≥n Completa

### Gu√≠as Estrat√©gicas
- [Mapa de features Exadata ‚Üí Spark](docs/exadata-feature-map.md)
- [Checklist de migraci√≥n](docs/migration-checklist.md)
- [Estrategia de validaci√≥n](docs/validation-strategy.md)
- [Cloud Deployment Guide](docs/cloud-deployment-guide.md)

### Gu√≠as de Desarrollo
- [PySpark Best Practices](docs/pyspark-best-practices.md)
- [Scala Spark Patterns](docs/scala-spark-patterns.md)
- [SQL vs DataFrame API](docs/sql-vs-dataframe-api.md)
- [Performance & Tuning](docs/spark-performance-tuning.md)

### Deployment por Cloud Provider
- [AWS EMR Setup](cloud/aws/emr-setup.md)
- [Azure Databricks Setup](cloud/azure/databricks-setup.md)
- [GCP Dataproc Setup](cloud/gcp/dataproc-setup.md)

---

## üõ†Ô∏è Stack Tecnol√≥gico

| Componente | Opciones Soportadas |
|-----------|---------------------|
| **Compute** | Spark 3.5+ (local, EMR, Databricks, Dataproc) |
| **Lenguajes** | SQL, Python 3.10+, Scala 2.12+ |
| **File Formats** | Parquet, Delta Lake, Iceberg |
| **Storage** | S3, ADLS Gen2, GCS, HDFS |
| **Notebooks** | Jupyter, Databricks, Zeppelin |

---

## üìä Comparativa Oracle vs Spark

| Feature Oracle Exadata | Equivalente Spark | Caso |
|------------------------|-------------------|------|
| Smart Scan / Storage Offload | Partition pruning + predicate pushdown | [02](cases/02-smart-scan-filter-pushdown/) |
| Storage Indexes | Data skipping + Z-ORDER | [04](cases/04-indexes-vs-layout/) |
| Bloom Filter Joins | Broadcast joins + AQE | [05](cases/05-star-joins-bloom/) |
| HCC (Columnar Compression) | Parquet/ORC compression | [02](cases/02-smart-scan-filter-pushdown/) |
| Materialized Views | Delta tables + incremental refresh | [06](cases/06-materialized-views/) |
| Result Cache | `CACHE TABLE` / persist | [07](cases/07-result-cache/) |
| Flashback Query | Delta/Iceberg time travel | [08](cases/08-flashback-time-travel/) |
| Parallel Hints | AQE + partition tuning | [01](cases/01-hints-parallel/) |

---

## ü§ù Contribuir

¬°Contribuciones son bienvenidas! Ver [CONTRIBUTING.md](CONTRIBUTING.md) para detalles.

Para agregar un nuevo caso:
1. Crea carpeta `cases/NN-nombre-caso/`
2. Incluye 4 implementaciones: Oracle SQL, Spark SQL, PySpark, Scala
3. Agrega datos de prueba y notebook
4. Documenta en README con comparativa

---

## üìù Nota sobre Lakehouse Formats

Este repositorio es neutral respecto al formato de lakehouse:
- **Delta Lake**: Soporte completo para MERGE, time travel, ACID
- **Iceberg**: Alternativas equivalentes para todas las features
- **Parquet plano**: Algunas capacidades requieren jobs adicionales

---

## üìÑ Licencia

MIT
