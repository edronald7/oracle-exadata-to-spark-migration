# Cloud Deployment Guide

GuÃ­a estratÃ©gica para deployment de Spark en AWS, Azure y GCP.

---

## ğŸŒ¥ï¸ Comparativa de Plataformas

| Aspecto | AWS | Azure | GCP |
|---------|-----|-------|-----|
| **Servicio Managed Spark** | EMR | Databricks / Synapse | Dataproc |
| **Serverless Option** | Glue | Synapse Serverless | Dataproc Serverless |
| **Storage Nativo** | S3 | ADLS Gen2 | GCS |
| **Data Warehouse** | Redshift | Synapse DW | BigQuery |
| **Lakehouse Format** | Delta/Iceberg/Hudi | Delta (Databricks native) | Iceberg/BigLake |
| **Notebooks** | EMR Notebooks, SageMaker | Databricks, Synapse Notebooks | Vertex AI, Dataproc Notebooks |
| **Precio Compute** | $$ | $$$ (Databricks premium) | $ (mÃ¡s econÃ³mico) |
| **Madurez Spark** | â­â­â­â­â­ | â­â­â­â­â­ (Databricks) | â­â­â­â­ |

---

## ğŸ—ï¸ Arquitecturas de Referencia

### Arquitectura 1: Batch ETL (ComÃºn en migraciones Oracle)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Oracle Exadataâ”‚
â”‚  (source)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ CDC / Batch Export
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Landing Zone (Bronze)                    â”‚
â”‚  S3/ADLS/GCS: raw Parquet files          â”‚
â”‚  Partitioned by: load_date                â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼  Spark Job (EMR/Databricks/Dataproc)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Silver Layer                             â”‚
â”‚  Cleansed, validated, deduplicated        â”‚
â”‚  Partitioned by: business_date            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼  Spark Aggregations
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gold Layer                               â”‚
â”‚  Business aggregations, Marts             â”‚
â”‚  Optimized for queries                    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Consumption                              â”‚
â”‚  - BI Tools (Tableau, Power BI)          â”‚
â”‚  - Ad-hoc queries (SQL)                   â”‚
â”‚  - ML models                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Arquitectura 2: Real-time + Batch (Kappa Architecture)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Oracle CDC   â”‚         â”‚ App Events   â”‚
â”‚ (GoldenGate) â”‚         â”‚ (API/Kafka)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                        â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Kafka/Kinesisâ”‚
         â”‚ EventHub     â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚
        â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Spark       â”‚    â”‚ Cold Storage â”‚
â”‚Streaming   â”‚    â”‚ S3/ADLS/GCS  â”‚
â”‚(real-time) â”‚    â”‚ (batch)      â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Delta/Icebergâ”‚
       â”‚ Unified View â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â˜ï¸ AWS: Amazon EMR

### CuÃ¡ndo usar EMR

âœ… **Ventajas**:
- Control completo del cluster (versiones, configs, dependencies)
- IntegraciÃ³n nativa con S3, Glue Catalog, Athena
- Costo eficiente (spot instances, autoscaling)
- Soporte para Hive, Presto, Flink, ademÃ¡s de Spark

âš ï¸ **Desventajas**:
- MÃ¡s operacional (gestiÃ³n de clusters, upgrades)
- Notebooks menos potentes que Databricks
- Requiere mÃ¡s expertise de infraestructura

### Setup Recomendado

```bash
# Crear cluster EMR con Spark 3.5
aws emr create-cluster \
  --name "Oracle-Spark-Migration" \
  --release-label emr-7.0.0 \
  --applications Name=Spark Name=Hadoop Name=Hive Name=JupyterEnterpriseGateway \
  --instance-type r5.4xlarge \
  --instance-count 5 \
  --ec2-attributes KeyName=mykey,SubnetId=subnet-xxx \
  --use-default-roles \
  --configurations file://configs/spark-config.json \
  --bootstrap-actions Path=s3://bucket/bootstrap.sh \
  --auto-scaling-role EMR_AutoScaling_DefaultRole \
  --steps file://steps/initial-load.json
```

**spark-config.json**:
```json
[
  {
    "Classification": "spark-defaults",
    "Properties": {
      "spark.sql.adaptive.enabled": "true",
      "spark.sql.adaptive.coalescePartitions.enabled": "true",
      "spark.sql.adaptive.skewJoin.enabled": "true",
      "spark.sql.shuffle.partitions": "400",
      "spark.sql.files.maxPartitionBytes": "134217728",
      "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
      "spark.dynamicAllocation.enabled": "true",
      "spark.shuffle.service.enabled": "true"
    }
  },
  {
    "Classification": "spark-hive-site",
    "Properties": {
      "hive.metastore.client.factory.class": "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory"
    }
  }
]
```

### Arquitectura de Datos en S3

```
s3://company-datalake/
â”œâ”€â”€ bronze/                    # Raw data from Oracle
â”‚   â”œâ”€â”€ fact_sales/
â”‚   â”‚   â””â”€â”€ load_date=2025-01-12/
â”‚   â”‚       â””â”€â”€ part-00000.parquet
â”‚   â””â”€â”€ dim_region/
â”‚       â””â”€â”€ snapshot=2025-01-12/
â”œâ”€â”€ silver/                    # Cleansed data
â”‚   â”œâ”€â”€ fact_sales/
â”‚   â”‚   â””â”€â”€ sale_date=2025-01-01/
â”‚   â”‚       â””â”€â”€ part-00000.parquet
â”‚   â””â”€â”€ dim_region/
â”‚       â””â”€â”€ region_id=1/
â”œâ”€â”€ gold/                      # Business aggregations
â”‚   â”œâ”€â”€ sales_by_region_daily/
â”‚   â”‚   â””â”€â”€ report_date=2025-01-12/
â”‚   â””â”€â”€ customer_360/
â””â”€â”€ artifacts/                 # Scripts, JARs
    â”œâ”€â”€ jobs/
    â”‚   â””â”€â”€ case01_hints_parallel.py
    â””â”€â”€ jars/
        â””â”€â”€ spark-migration-cases.jar
```

### Cost Optimization

```bash
# Usar spot instances para core/task nodes
aws emr create-cluster \
  --instance-groups \
    InstanceGroupType=MASTER,InstanceType=r5.2xlarge,InstanceCount=1 \
    InstanceGroupType=CORE,InstanceType=r5.4xlarge,InstanceCount=2,BidPrice=OnDemandPrice \
    InstanceGroupType=TASK,InstanceType=r5.4xlarge,InstanceCount=8,BidPrice=0.40

# Autoscaling basado en YARN metrics
aws emr put-auto-scaling-policy \
  --cluster-id j-XXXXX \
  --instance-group-id ig-XXXXX \
  --auto-scaling-policy file://autoscaling-policy.json
```

---

## â˜ï¸ Azure: Databricks

### CuÃ¡ndo usar Databricks

âœ… **Ventajas**:
- Notebooks colaborativos potentes (mejor experiencia de desarrollo)
- Delta Lake nativo (ACID, time travel, merge)
- Unity Catalog para governance
- Workflows integrados (orquestaciÃ³n)
- MLflow integrado para ML

âš ï¸ **Desventajas**:
- MÃ¡s costoso (DBU + compute)
- Vendor lock-in parcial
- Menos control sobre infraestructura

### Setup Recomendado

```bash
# Crear workspace con Azure CLI
az databricks workspace create \
  --resource-group rg-spark-migration \
  --name spark-migration-workspace \
  --location eastus \
  --sku premium

# Configurar con Terraform (recomendado)
```

**Terraform ejemplo**:
```hcl
resource "azurerm_databricks_workspace" "main" {
  name                = "spark-migration-workspace"
  resource_group_name = azurerm_resource_group.main.name
  location            = azurerm_resource_group.main.location
  sku                 = "premium"
}

resource "databricks_cluster" "shared_autoscaling" {
  cluster_name            = "Oracle Migration Cluster"
  spark_version           = "14.3.x-scala2.12"  # Spark 3.5.0
  node_type_id            = "Standard_DS4_v2"
  autotermination_minutes = 20
  
  autoscale {
    min_workers = 2
    max_workers = 10
  }
  
  spark_conf = {
    "spark.databricks.delta.preview.enabled"          = "true"
    "spark.databricks.delta.optimizeWrite.enabled"    = "true"
    "spark.databricks.delta.autoCompact.enabled"      = "true"
    "spark.sql.adaptive.enabled"                      = "true"
  }
  
  library {
    maven {
      coordinates = "io.delta:delta-core_2.12:2.4.0"
    }
  }
}
```

### Delta Lake Best Practices

```python
# Crear tabla Delta con optimizaciones
spark.sql("""
    CREATE TABLE IF NOT EXISTS fact_sales_delta
    USING DELTA
    PARTITIONED BY (sale_date)
    LOCATION 'abfss://container@storage.dfs.core.windows.net/gold/fact_sales'
    TBLPROPERTIES (
      'delta.autoOptimize.optimizeWrite' = 'true',
      'delta.autoOptimize.autoCompact' = 'true',
      'delta.deletedFileRetentionDuration' = 'interval 7 days'
    )
    AS SELECT * FROM bronze.fact_sales
""")

# OPTIMIZE + ZORDER para queries frecuentes
spark.sql("""
    OPTIMIZE fact_sales_delta
    ZORDER BY (region_id, customer_id)
""")

# Time travel (audit)
spark.sql("""
    SELECT * FROM fact_sales_delta
    VERSION AS OF 10
    WHERE sale_date = '2025-01-12'
""")

# MERGE para SCD (upsert)
spark.sql("""
    MERGE INTO dim_customer_delta target
    USING staging_customers source
    ON target.customer_id = source.customer_id
    WHEN MATCHED THEN UPDATE SET *
    WHEN NOT MATCHED THEN INSERT *
""")
```

### Unity Catalog para Governance

```python
# Crear catÃ¡logo y esquemas
spark.sql("CREATE CATALOG IF NOT EXISTS oracle_migration")
spark.sql("USE CATALOG oracle_migration")
spark.sql("CREATE SCHEMA IF NOT EXISTS bronze")
spark.sql("CREATE SCHEMA IF NOT EXISTS silver")
spark.sql("CREATE SCHEMA IF NOT EXISTS gold")

# Permisos granulares
spark.sql("""
    GRANT SELECT ON SCHEMA oracle_migration.gold TO `analysts@company.com`
""")

spark.sql("""
    GRANT ALL PRIVILEGES ON SCHEMA oracle_migration.bronze TO `engineers@company.com`
""")
```

---

## â˜ï¸ GCP: Dataproc

### CuÃ¡ndo usar Dataproc

âœ… **Ventajas**:
- MÃ¡s econÃ³mico que EMR/Databricks
- RÃ¡pido aprovisionamiento (< 90 segundos)
- IntegraciÃ³n nativa con BigQuery
- Autoscaling potente
- Serverless option (Dataproc Serverless)

âš ï¸ **Desventajas**:
- Notebooks menos maduros
- Menos features de governance que Databricks
- Delta Lake requiere configuraciÃ³n adicional

### Setup Recomendado

```bash
# Crear cluster Dataproc
gcloud dataproc clusters create oracle-migration-cluster \
  --region=us-central1 \
  --zone=us-central1-a \
  --master-machine-type=n2-highmem-4 \
  --master-boot-disk-size=100 \
  --num-workers=4 \
  --worker-machine-type=n2-highmem-8 \
  --worker-boot-disk-size=100 \
  --image-version=2.1-debian11 \
  --optional-components=JUPYTER,ZEPPELIN \
  --enable-component-gateway \
  --autoscaling-policy=my-autoscaling-policy \
  --properties=spark:spark.sql.adaptive.enabled=true,spark:spark.sql.shuffle.partitions=400
```

**Autoscaling policy**:
```bash
gcloud dataproc autoscaling-policies import my-autoscaling-policy \
  --source=autoscaling-policy.yaml \
  --region=us-central1
```

**autoscaling-policy.yaml**:
```yaml
workerConfig:
  minInstances: 2
  maxInstances: 20
  weight: 1
secondaryWorkerConfig:
  minInstances: 0
  maxInstances: 50
  weight: 1
basicAlgorithm:
  yarnConfig:
    gracefulDecommissionTimeout: 1h
    scaleUpFactor: 1.0
    scaleDownFactor: 1.0
    scaleUpMinWorkerFraction: 0.0
    scaleDownMinWorkerFraction: 0.0
```

### IntegraciÃ³n con BigQuery

```python
# Leer de BigQuery (pushdown de predicados)
df = spark.read \
    .format("bigquery") \
    .option("table", "project.dataset.oracle_exported_sales") \
    .option("filter", "sale_date >= '2025-01-01'") \
    .load()

# Procesar en Spark
result = df.filter(df.status == "ACTIVE") \
    .groupBy("region_id") \
    .agg({"amount": "sum"})

# Escribir a BigQuery
result.write \
    .format("bigquery") \
    .option("table", "project.dataset.sales_aggregated") \
    .option("temporaryGcsBucket", "temp-bucket") \
    .mode("overwrite") \
    .save()
```

### Dataproc Serverless (para jobs batch)

```bash
# No requiere cluster pre-existente
gcloud dataproc batches submit pyspark \
  gs://bucket/jobs/case01_hints_parallel.py \
  --region=us-central1 \
  --batch=case01-batch-$(date +%s) \
  --deps-bucket=gs://bucket/dependencies \
  --properties=spark.sql.adaptive.enabled=true \
  --service-account=spark-jobs@project.iam.gserviceaccount.com \
  -- --input gs://bucket/data/sales --output gs://bucket/results
```

---

## ğŸ”„ Estrategia Multi-Cloud

### CuÃ¡ndo considerar multi-cloud

âœ… **Casos de uso**:
- RegulaciÃ³n (data residency)
- Disaster recovery
- Aprovechar servicios especÃ­ficos de cada cloud
- NegociaciÃ³n de precios

### Arquitectura Portable

```python
# config.py - AbstracciÃ³n de storage
import os
from enum import Enum

class CloudProvider(Enum):
    AWS = "aws"
    AZURE = "azure"
    GCP = "gcp"

class StoragePaths:
    def __init__(self, provider: CloudProvider, bucket: str):
        self.provider = provider
        self.bucket = bucket
    
    @property
    def bronze_path(self):
        if self.provider == CloudProvider.AWS:
            return f"s3://{self.bucket}/bronze/"
        elif self.provider == CloudProvider.AZURE:
            return f"abfss://{self.bucket}@storage.dfs.core.windows.net/bronze/"
        elif self.provider == CloudProvider.GCP:
            return f"gs://{self.bucket}/bronze/"
    
    @property
    def silver_path(self):
        # Similar pattern
        pass

# Uso
provider = CloudProvider(os.getenv("CLOUD_PROVIDER", "aws"))
storage = StoragePaths(provider, "my-datalake")

df = spark.read.parquet(storage.bronze_path + "fact_sales")
```

---

## ğŸ“Š Comparativa de Costos (Estimado)

### Escenario: Procesamiento diario de 1TB

| Cloud | Servicio | Config | Costo/dÃ­a | Costo/mes |
|-------|----------|--------|-----------|-----------|
| AWS | EMR | 10 x r5.4xlarge (16h) | $80 | $2,400 |
| AWS | Glue | 100 DPU-hours | $44 | $1,320 |
| Azure | Databricks | Standard, 10 workers | $120 | $3,600 |
| GCP | Dataproc | 10 x n2-highmem-8 (16h) | $60 | $1,800 |
| GCP | Dataproc Serverless | 100 DCU-hours | $40 | $1,200 |

**Notas**:
- Precios aproximados (2025)
- No incluye storage (S3/ADLS/GCS)
- Spot/preemptible puede reducir 60-80%
- Databricks incluye UI premium

---

## âœ… Checklist de Deployment

### Pre-deployment
- [ ] Storage account creado (S3/ADLS/GCS)
- [ ] IAM roles/service accounts configurados
- [ ] VPC/VNet/subnet configurados
- [ ] Data catalog setup (Glue/Hive/Unity)
- [ ] Secrets management (Secrets Manager/Key Vault)

### Deployment
- [ ] Cluster/workspace aprovisionado
- [ ] Spark configs optimizados
- [ ] Autoscaling configurado
- [ ] Monitoreo habilitado (CloudWatch/Monitor/Cloud Logging)
- [ ] Jobs desplegados
- [ ] Notebooks importados

### Post-deployment
- [ ] Smoke tests ejecutados
- [ ] Performance baseline establecido
- [ ] Alertas configuradas
- [ ] DocumentaciÃ³n actualizada
- [ ] Team capacity training completado

---

## ğŸ“š Recursos por Cloud

### AWS
- [EMR Best Practices](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan.html)
- [Glue ETL Guide](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-glue-arguments.html)
- [S3 Performance](https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html)

### Azure
- [Databricks Best Practices](https://docs.databricks.com/optimizations/index.html)
- [Delta Lake Guide](https://docs.databricks.com/delta/index.html)
- [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html)

### GCP
- [Dataproc Best Practices](https://cloud.google.com/dataproc/docs/guides)
- [BigQuery Spark Integration](https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example)
- [Dataproc Serverless](https://cloud.google.com/dataproc-serverless/docs)
