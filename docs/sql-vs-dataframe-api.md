# SQL vs DataFrame API: Gu√≠a de Decisi√≥n

Comparativa pr√°ctica entre Spark SQL y DataFrame API (PySpark/Scala) para ayudarte a elegir el enfoque correcto.

---

## üéØ Resumen Ejecutivo

| Criterio | Spark SQL | DataFrame API (PySpark/Scala) |
|----------|-----------|-------------------------------|
| **Curva de aprendizaje** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê M√≠nima (si sabes SQL) | ‚≠ê‚≠ê‚≠ê Moderada |
| **Flexibilidad** | ‚≠ê‚≠ê‚≠ê Limitada a SQL | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Programaci√≥n completa |
| **Testing** | ‚≠ê‚≠ê Complicado | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê F√°cil (pytest, ScalaTest) |
| **Refactoring** | ‚≠ê‚≠ê Dif√≠cil (strings) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê F√°cil (IDE support) |
| **Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Equivalente | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Equivalente |
| **Debugging** | ‚≠ê‚≠ê‚≠ê Errores en runtime | ‚≠ê‚≠ê‚≠ê‚≠ê Stack traces claros |

### Recomendaci√≥n R√°pida

```
An√°lisis exploratorio       ‚Üí Spark SQL
Queries simples             ‚Üí Spark SQL  
Pipelines de producci√≥n     ‚Üí DataFrame API
L√≥gica compleja/condicional ‚Üí DataFrame API
ETL con transformaciones    ‚Üí DataFrame API
```

---

## üìä Ejemplos Lado a Lado

### Ejemplo 1: Filtro Simple

**Spark SQL**:
```sql
SELECT customer_id, amount
FROM sales
WHERE sale_date >= '2025-01-01'
  AND status = 'ACTIVE'
```

**PySpark**:
```python
from pyspark.sql.functions import col

df = spark.table("sales") \
    .select("customer_id", "amount") \
    .filter(
        (col("sale_date") >= "2025-01-01") &
        (col("status") == "ACTIVE")
    )
```

**Scala**:
```scala
import spark.implicits._

val df = spark.table("sales")
  .select($"customer_id", $"amount")
  .filter($"sale_date" >= "2025-01-01" && $"status" === "ACTIVE")
```

**An√°lisis**: 
- SQL es m√°s conciso y familiar
- DataFrame API requiere imports y sintaxis espec√≠fica
- **Ganador**: SQL para este caso simple

---

### Ejemplo 2: Agregaci√≥n con Join

**Spark SQL**:
```sql
SELECT 
  r.region_name,
  COUNT(*) as transaction_count,
  SUM(s.amount) as total_amount,
  AVG(s.amount) as avg_amount
FROM sales s
JOIN regions r ON s.region_id = r.region_id
WHERE s.sale_date >= '2025-01-01'
GROUP BY r.region_name
HAVING SUM(s.amount) > 10000
ORDER BY total_amount DESC
```

**PySpark**:
```python
from pyspark.sql import functions as F

result = spark.table("sales") \
    .filter(F.col("sale_date") >= "2025-01-01") \
    .join(spark.table("regions"), "region_id") \
    .groupBy("region_name") \
    .agg(
        F.count("*").alias("transaction_count"),
        F.sum("amount").alias("total_amount"),
        F.avg("amount").alias("avg_amount")
    ) \
    .filter(F.col("total_amount") > 10000) \
    .orderBy(F.desc("total_amount"))
```

**Scala**:
```scala
import org.apache.spark.sql.functions._

val result = spark.table("sales")
  .filter($"sale_date" >= "2025-01-01")
  .join(spark.table("regions"), Seq("region_id"))
  .groupBy("region_name")
  .agg(
    count("*").as("transaction_count"),
    sum("amount").as("total_amount"),
    avg("amount").as("avg_amount")
  )
  .filter($"total_amount" > 10000)
  .orderBy(desc("total_amount"))
```

**An√°lisis**: 
- SQL sigue siendo m√°s legible
- DataFrame API permite composici√≥n incremental
- **Ganador**: SQL para legibilidad, DataFrame para reusabilidad

---

### Ejemplo 3: L√≥gica Condicional Compleja

**Spark SQL**:
```sql
SELECT
  customer_id,
  amount,
  CASE 
    WHEN amount > 1000 AND loyalty_years > 5 THEN 'PLATINUM'
    WHEN amount > 500 AND loyalty_years > 3 THEN 'GOLD'
    WHEN amount > 100 OR loyalty_years > 1 THEN 'SILVER'
    ELSE 'BRONZE'
  END as tier,
  CASE
    WHEN tier = 'PLATINUM' THEN amount * 0.20
    WHEN tier = 'GOLD' THEN amount * 0.15
    WHEN tier = 'SILVER' THEN amount * 0.10
    ELSE amount * 0.05
  END as discount_amount
FROM sales
```

**PySpark** (mejor approach):
```python
from pyspark.sql.functions import when, col

def calculate_tier(df):
    """Calcula tier basado en l√≥gica de negocio"""
    return df.withColumn("tier",
        when((col("amount") > 1000) & (col("loyalty_years") > 5), "PLATINUM")
        .when((col("amount") > 500) & (col("loyalty_years") > 3), "GOLD")
        .when((col("amount") > 100) | (col("loyalty_years") > 1), "SILVER")
        .otherwise("BRONZE")
    )

def calculate_discount(df):
    """Calcula descuento basado en tier"""
    discount_rates = {"PLATINUM": 0.20, "GOLD": 0.15, "SILVER": 0.10, "BRONZE": 0.05}
    
    discount_expr = when(col("tier") == "PLATINUM", col("amount") * 0.20)
    for tier, rate in list(discount_rates.items())[1:]:
        discount_expr = discount_expr.when(col("tier") == tier, col("amount") * rate)
    
    return df.withColumn("discount_amount", discount_expr.otherwise(col("amount") * 0.05))

# Uso: composici√≥n de transformaciones
result = (spark.table("sales")
    .transform(calculate_tier)
    .transform(calculate_discount)
)
```

**Scala**:
```scala
import org.apache.spark.sql.functions._
import org.apache.spark.sql.DataFrame

def calculateTier(df: DataFrame): DataFrame = {
  df.withColumn("tier",
    when($"amount" > 1000 && $"loyalty_years" > 5, "PLATINUM")
    .when($"amount" > 500 && $"loyalty_years" > 3, "GOLD")
    .when($"amount" > 100 || $"loyalty_years" > 1, "SILVER")
    .otherwise("BRONZE")
  )
}

def calculateDiscount(df: DataFrame): DataFrame = {
  df.withColumn("discount_amount",
    when($"tier" === "PLATINUM", $"amount" * 0.20)
    .when($"tier" === "GOLD", $"amount" * 0.15)
    .when($"tier" === "SILVER", $"amount" * 0.10)
    .otherwise($"amount" * 0.05)
  )
}

// Uso
val result = spark.table("sales")
  .transform(calculateTier)
  .transform(calculateDiscount)
```

**An√°lisis**: 
- SQL funciona pero es monol√≠tico y dif√≠cil de testear
- DataFrame API permite modularizar l√≥gica en funciones reutilizables
- M√°s f√°cil de testear y mantener
- **Ganador**: DataFrame API

---

### Ejemplo 4: Window Functions

**Spark SQL**:
```sql
SELECT
  customer_id,
  sale_date,
  amount,
  SUM(amount) OVER (
    PARTITION BY customer_id 
    ORDER BY sale_date
    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
  ) as running_total,
  ROW_NUMBER() OVER (
    PARTITION BY customer_id 
    ORDER BY amount DESC
  ) as amount_rank
FROM sales
```

**PySpark**:
```python
from pyspark.sql import Window
from pyspark.sql.functions import sum as _sum, row_number

window_running = Window.partitionBy("customer_id") \
    .orderBy("sale_date") \
    .rowsBetween(Window.unboundedPreceding, Window.currentRow)

window_rank = Window.partitionBy("customer_id") \
    .orderBy(col("amount").desc())

result = spark.table("sales") \
    .withColumn("running_total", _sum("amount").over(window_running)) \
    .withColumn("amount_rank", row_number().over(window_rank))
```

**Scala**:
```scala
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._

val windowRunning = Window
  .partitionBy("customer_id")
  .orderBy("sale_date")
  .rowsBetween(Window.unboundedPreceding, Window.currentRow)

val windowRank = Window
  .partitionBy("customer_id")
  .orderBy($"amount".desc)

val result = spark.table("sales")
  .withColumn("running_total", sum("amount").over(windowRunning))
  .withColumn("amount_rank", row_number().over(windowRank))
```

**An√°lisis**: 
- SQL es m√°s conciso para window functions
- DataFrame API permite reutilizar definiciones de windows
- **Empate**: depende del caso de uso

---

## üîÑ Interoperabilidad: Usar Ambos

La mejor estrategia es **usar ambos seg√∫n convenga**.

### Pattern: SQL para queries complejas, DataFrame para transformaciones

```python
# Leer con SQL (conveniente)
df = spark.sql("""
    SELECT 
      s.customer_id,
      s.amount,
      r.region_name,
      p.product_category
    FROM sales s
    JOIN regions r ON s.region_id = r.region_id
    JOIN products p ON s.product_id = p.product_id
    WHERE s.sale_date >= '2025-01-01'
""")

# Transformar con DataFrame API (modular)
def add_business_logic(df):
    return df.withColumn("tier",
        when(col("amount") > 1000, "HIGH")
        .when(col("amount") > 100, "MEDIUM")
        .otherwise("LOW")
    )

result = df.transform(add_business_logic)

# Volver a registrar como temp view para usar en SQL
result.createOrReplaceTempView("enriched_sales")

# Agregar con SQL (familiar para analistas)
final = spark.sql("""
    SELECT region_name, tier, COUNT(*), SUM(amount)
    FROM enriched_sales
    GROUP BY region_name, tier
""")
```

### Pattern: Funciones UDF compartidas

```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

# Definir UDF
@udf(returnType=StringType())
def categorize_amount(amount):
    if amount > 1000:
        return "HIGH"
    elif amount > 100:
        return "MEDIUM"
    else:
        return "LOW"

# Registrar para SQL
spark.udf.register("categorize_amount", categorize_amount)

# Usar en SQL
spark.sql("""
    SELECT customer_id, categorize_amount(amount) as category
    FROM sales
""")

# Usar en DataFrame API
df = spark.table("sales") \
    .withColumn("category", categorize_amount(col("amount")))
```

---

## ‚úÖ Matriz de Decisi√≥n

### Usa **Spark SQL** cuando:

‚úÖ **An√°lisis exploratorio** / ad-hoc queries  
‚úÖ **Analistas sin experiencia en programaci√≥n**  
‚úÖ **Migrando queries Oracle existentes**  
‚úÖ **Query simple** (select, filter, join, group by)  
‚úÖ **Reportes con sintaxis SQL familiar**  
‚úÖ **Prototipado r√°pido**  
‚úÖ **Notebooks interactivos**  

### Usa **DataFrame API** cuando:

‚úÖ **Pipelines de producci√≥n**  
‚úÖ **L√≥gica condicional compleja**  
‚úÖ **Testing unitario requerido**  
‚úÖ **Refactoring frecuente**  
‚úÖ **Composici√≥n de transformaciones modulares**  
‚úÖ **Integraci√≥n con c√≥digo Python/Scala**  
‚úÖ **Type safety (Scala Datasets)**  
‚úÖ **CI/CD pipelines**  

---

## üß™ Testing: Ventaja del DataFrame API

### Testing SQL (complicado)

```python
def test_sales_query():
    # Dif√≠cil: query como string
    query = """
        SELECT region, SUM(amount)
        FROM sales
        WHERE date >= '2025-01-01'
        GROUP BY region
    """
    result = spark.sql(query)
    
    # Si cambias el esquema de sales, test rompe en runtime
    assert result.count() > 0
```

### Testing DataFrame API (f√°cil)

```python
import pytest
from pyspark.sql import SparkSession

@pytest.fixture
def spark():
    return SparkSession.builder.master("local[2]").getOrCreate()

@pytest.fixture
def sample_sales(spark):
    data = [
        ("region1", "2025-01-15", 100.0),
        ("region2", "2025-01-20", 200.0),
    ]
    return spark.createDataFrame(data, ["region", "date", "amount"])

def aggregate_by_region(df):
    """Funci√≥n testeable"""
    return df.groupBy("region").agg({"amount": "sum"})

def test_aggregate_by_region(spark, sample_sales):
    # Test claro y espec√≠fico
    result = aggregate_by_region(sample_sales)
    
    assert result.count() == 2
    
    region1_total = result.filter("region = 'region1'").first()["sum(amount)"]
    assert region1_total == 100.0
```

---

## üìä Performance: Son Equivalentes

**Mito**: "DataFrame API es m√°s r√°pido que SQL"  
**Realidad**: Ambos usan el mismo Catalyst Optimizer y generan el mismo plan f√≠sico.

### Prueba:

```python
# SQL
result_sql = spark.sql("""
    SELECT region, SUM(amount)
    FROM sales
    GROUP BY region
""")

# DataFrame API
result_df = spark.table("sales") \
    .groupBy("region") \
    .agg({"amount": "sum"})

# Mismo plan f√≠sico
result_sql.explain()
result_df.explain()
# Output id√©ntico
```

**Conclusi√≥n**: Elige basado en mantenibilidad, no en performance.

---

## üéì Estrategia de Adopci√≥n

### Para Equipos de Analistas
1. Comienza con 100% SQL
2. Introduce DataFrame API gradualmente para l√≥gica compleja
3. Capacita en PySpark basics (filter, select, withColumn)
4. Mant√©n SQL para queries de reporte

### Para Equipos de Engineering
1. Usa DataFrame API como default
2. SQL solo para queries simples en notebooks
3. Todas las transformaciones en funciones testeables
4. CI/CD con pytest/ScalaTest

### H√≠brido (Recomendado)
```
Exploraci√≥n ‚Üí SQL
L√≥gica ‚Üí DataFrame API  
Reportes ‚Üí SQL
Testing ‚Üí DataFrame API
Producci√≥n ‚Üí DataFrame API
```

---

## üìö Cheat Sheet

| Operaci√≥n | SQL | PySpark | Scala |
|-----------|-----|---------|-------|
| **Select** | `SELECT col1, col2` | `.select("col1", "col2")` | `.select($"col1", $"col2")` |
| **Filter** | `WHERE col > 10` | `.filter(col("col") > 10)` | `.filter($"col" > 10)` |
| **Join** | `FROM a JOIN b ON ...` | `.join(b, "key")` | `.join(b, Seq("key"))` |
| **Group By** | `GROUP BY col` | `.groupBy("col")` | `.groupBy("col")` |
| **Aggregate** | `SUM(amount)` | `.agg(sum("amount"))` | `.agg(sum("amount"))` |
| **Order** | `ORDER BY col DESC` | `.orderBy(desc("col"))` | `.orderBy($"col".desc)` |
| **Limit** | `LIMIT 10` | `.limit(10)` | `.limit(10)` |
| **Window** | `OVER (PARTITION BY ...)` | `.over(window_spec)` | `.over(windowSpec)` |

---

## üéØ Conclusi√≥n

**No hay un "ganador" absoluto**. La mejor pr√°ctica es:

1. **Entender ambos enfoques**
2. **Elegir seg√∫n el contexto**:
   - SQL para queries simples y an√°lisis exploratorio
   - DataFrame API para pipelines de producci√≥n
3. **Combinar ambos** cuando tenga sentido
4. **Priorizar mantenibilidad** sobre preferencias personales

**Regla de oro**: Si vas a poner el c√≥digo en producci√≥n y necesita tests ‚Üí DataFrame API. Si es an√°lisis exploratorio ‚Üí SQL.
