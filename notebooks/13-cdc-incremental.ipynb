{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Caso 13: CDC e Ingesta Incremental\n",
        "\n",
        "**Objetivo**: Aprender a implementar Change Data Capture (CDC) e ingesta incremental desde Oracle a Delta Lake.\n",
        "\n",
        "## üìã Contenido\n",
        "\n",
        "1. Setup inicial\n",
        "2. Generar datos CDC sint√©ticos\n",
        "3. Implementar pipeline incremental con deduplicaci√≥n\n",
        "4. MERGE a Delta Lake\n",
        "5. Validaci√≥n de resultados\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Prerequisitos\n",
        "\n",
        "- Spark 3.x configurado\n",
        "- Delta Lake instalado\n",
        "- Datos de prueba generados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Inicializar Spark Session con Delta Lake\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Caso 13: CDC Incremental\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"‚úÖ Spark Session creado con Delta Lake\")\n",
        "print(f\"   Version: {spark.version}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 1: Generar Datos CDC Sint√©ticos\n",
        "\n",
        "Vamos a simular datos de Change Data Capture como si vinieran de Oracle GoldenGate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generar datos CDC sint√©ticos\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# D√≠a 1: Initial load (1000 customers)\n",
        "print(\"üìä Generando initial load (1000 customers)...\")\n",
        "initial = spark.range(1, 1001).selectExpr(\n",
        "    \"id as customer_id\",\n",
        "    \"concat('Customer ', id) as name\",\n",
        "    \"concat('customer', id, '@example.com') as email\",\n",
        "    \"concat('+1-555-', lpad(id, 4, '0')) as phone\",\n",
        "    \"'I' as op_type\",\n",
        "    \"timestamp('2026-01-10 00:00:00') as op_ts\"\n",
        ")\n",
        "\n",
        "initial.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"/tmp/cdc_data/customers/date=2026-01-10\")\n",
        "\n",
        "print(f\"   ‚úÖ Initial load: {initial.count():,} registros\")\n",
        "\n",
        "# D√≠a 2: Updates (100 customers) + Deletes (10 customers)\n",
        "print(\"\\nüìä Generando cambios d√≠a 2...\")\n",
        "updates = spark.range(1, 101).selectExpr(\n",
        "    \"id as customer_id\",\n",
        "    \"concat('UPDATED Customer ', id) as name\",\n",
        "    \"concat('customer', id, '_new@example.com') as email\",\n",
        "    \"concat('+1-555-', lpad(id, 4, '0')) as phone\",\n",
        "    \"'U' as op_type\",\n",
        "    \"timestamp('2026-01-11 12:00:00') as op_ts\"\n",
        ")\n",
        "\n",
        "deletes = spark.range(991, 1001).selectExpr(\n",
        "    \"id as customer_id\",\n",
        "    \"cast(null as string) as name\",\n",
        "    \"cast(null as string) as email\",\n",
        "    \"cast(null as string) as phone\",\n",
        "    \"'D' as op_type\",\n",
        "    \"timestamp('2026-01-11 18:00:00') as op_ts\"\n",
        ")\n",
        "\n",
        "day2_changes = updates.union(deletes)\n",
        "day2_changes.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"/tmp/cdc_data/customers/date=2026-01-11\")\n",
        "\n",
        "print(f\"   ‚úÖ Updates: 100 registros\")\n",
        "print(f\"   ‚úÖ Deletes: 10 registros\")\n",
        "print(f\"   Total d√≠a 2: {day2_changes.count():,} cambios\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 2: Ver Estructura de Datos CDC\n",
        "\n",
        "Los datos CDC incluyen:\n",
        "- `customer_id`: ID del cliente\n",
        "- `name`, `email`, `phone`: Datos del cliente\n",
        "- `op_type`: Tipo de operaci√≥n (I=Insert, U=Update, D=Delete)\n",
        "- `op_ts`: Timestamp de la operaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ver datos CDC del d√≠a 2\n",
        "cdc_day2 = spark.read.parquet(\"/tmp/cdc_data/customers/date=2026-01-11\")\n",
        "\n",
        "print(\"üìä Datos CDC - D√≠a 2:\")\n",
        "print(f\"   Total registros: {cdc_day2.count()}\")\n",
        "\n",
        "print(\"\\nüîç Distribuci√≥n por tipo de operaci√≥n:\")\n",
        "cdc_day2.groupBy(\"op_type\").count().show()\n",
        "\n",
        "print(\"\\nüëÄ Ejemplos de cada tipo:\")\n",
        "print(\"\\n   Updates (U):\")\n",
        "cdc_day2.filter(\"op_type = 'U'\").show(3, truncate=False)\n",
        "\n",
        "print(\"   Deletes (D):\")\n",
        "cdc_day2.filter(\"op_type = 'D'\").show(3, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 3: Implementar Deduplicaci√≥n\n",
        "\n",
        "En CDC real, es posible tener m√∫ltiples cambios del mismo registro en el mismo batch.\n",
        "Debemos quedarnos con el cambio m√°s reciente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deduplicaci√≥n usando Window Function\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "print(\"üîÑ Aplicando deduplicaci√≥n...\")\n",
        "\n",
        "# Crear window particionado por customer_id, ordenado por timestamp descendente\n",
        "window_spec = Window.partitionBy(\"customer_id\").orderBy(col(\"op_ts\").desc())\n",
        "\n",
        "# Agregar row_number y filtrar solo el primer registro\n",
        "deduplicated = cdc_day2 \\\n",
        "    .withColumn(\"rn\", row_number().over(window_spec)) \\\n",
        "    .filter(col(\"rn\") == 1) \\\n",
        "    .drop(\"rn\")\n",
        "\n",
        "print(f\"   Before dedup: {cdc_day2.count():,} registros\")\n",
        "print(f\"   After dedup: {deduplicated.count():,} registros\")\n",
        "print(f\"   ‚úÖ Eliminados {cdc_day2.count() - deduplicated.count()} duplicados\")\n",
        "\n",
        "# Guardar deduplicados\n",
        "deduplicated.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"/tmp/cdc_data/customers_dedup/date=2026-01-11\")\n",
        "\n",
        "print(\"\\nüìä Distribuci√≥n despu√©s de dedup:\")\n",
        "deduplicated.groupBy(\"op_type\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 4: Crear Tabla Delta (Primera Carga)\n",
        "\n",
        "Primero cargamos el initial load a Delta Lake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Leer initial load y crear tabla Delta\n",
        "print(\"üì¶ Creando tabla Delta con initial load...\")\n",
        "\n",
        "initial_load = spark.read.parquet(\"/tmp/cdc_data/customers/date=2026-01-10\") \\\n",
        "    .filter(\"op_type = 'I'\") \\\n",
        "    .select(\n",
        "        \"customer_id\",\n",
        "        \"name\",\n",
        "        \"email\",\n",
        "        \"phone\",\n",
        "        col(\"op_ts\").alias(\"created_at\"),\n",
        "        col(\"op_ts\").alias(\"updated_at\")\n",
        "    )\n",
        "\n",
        "initial_load.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"/tmp/delta/customers\")\n",
        "\n",
        "print(f\"   ‚úÖ Tabla Delta creada con {initial_load.count():,} registros\")\n",
        "\n",
        "# Verificar\n",
        "delta_table = spark.read.format(\"delta\").load(\"/tmp/delta/customers\")\n",
        "print(f\"\\nüìä Estado de tabla Delta:\")\n",
        "print(f\"   Total registros: {delta_table.count():,}\")\n",
        "delta_table.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 5: MERGE - Aplicar Cambios Incrementales\n",
        "\n",
        "Ahora aplicamos los cambios del d√≠a 2 usando MERGE de Delta Lake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Leer cambios deduplicados\n",
        "print(\"üîÄ Ejecutando MERGE de cambios incrementales...\")\n",
        "\n",
        "cdc_changes = spark.read.parquet(\"/tmp/cdc_data/customers_dedup/date=2026-01-11\")\n",
        "\n",
        "# Obtener tabla Delta\n",
        "target_table = DeltaTable.forPath(spark, \"/tmp/delta/customers\")\n",
        "\n",
        "# MERGE operation\n",
        "target_table.alias(\"target\").merge(\n",
        "    cdc_changes.alias(\"source\"),\n",
        "    \"target.customer_id = source.customer_id\"\n",
        ").whenMatchedUpdate(\n",
        "    condition=\"source.op_type = 'U'\",\n",
        "    set={\n",
        "        \"name\": \"source.name\",\n",
        "        \"email\": \"source.email\",\n",
        "        \"phone\": \"source.phone\",\n",
        "        \"updated_at\": \"source.op_ts\"\n",
        "    }\n",
        ").whenMatchedDelete(\n",
        "    condition=\"source.op_type = 'D'\"\n",
        ").whenNotMatchedInsert(\n",
        "    condition=\"source.op_type IN ('I', 'U')\",\n",
        "    values={\n",
        "        \"customer_id\": \"source.customer_id\",\n",
        "        \"name\": \"source.name\",\n",
        "        \"email\": \"source.email\",\n",
        "        \"phone\": \"source.phone\",\n",
        "        \"created_at\": \"source.op_ts\",\n",
        "        \"updated_at\": \"source.op_ts\"\n",
        "    }\n",
        ").execute()\n",
        "\n",
        "print(\"   ‚úÖ MERGE completado\")\n",
        "\n",
        "# Verificar estado final\n",
        "final_state = spark.read.format(\"delta\").load(\"/tmp/delta/customers\")\n",
        "print(f\"\\nüìä Estado final de tabla Delta:\")\n",
        "print(f\"   Total registros: {final_state.count():,}\")\n",
        "print(f\"   Esperado: 990 (1000 initial - 10 deletes)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 6: Validaci√≥n de Resultados\n",
        "\n",
        "Verificar que los cambios se aplicaron correctamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validaci√≥n 1: Verificar UPDATEs\n",
        "print(\"‚úÖ Validaci√≥n 1: Verificar registros actualizados\")\n",
        "print(\"   Buscando registros con 'UPDATED' en el nombre...\")\n",
        "\n",
        "updated_records = final_state.filter(\"name LIKE 'UPDATED%'\")\n",
        "print(f\"   ‚úÖ Encontrados: {updated_records.count()} registros actualizados (esperado: 100)\")\n",
        "updated_records.show(5, truncate=False)\n",
        "\n",
        "# Validaci√≥n 2: Verificar DELETEs\n",
        "print(\"\\n‚úÖ Validaci√≥n 2: Verificar que registros fueron eliminados\")\n",
        "deleted_ids = list(range(991, 1001))\n",
        "remaining_deleted = final_state.filter(col(\"customer_id\").isin(deleted_ids))\n",
        "print(f\"   Registros con IDs 991-1000: {remaining_deleted.count()} (esperado: 0)\")\n",
        "if remaining_deleted.count() == 0:\n",
        "    print(\"   ‚úÖ DELETES aplicados correctamente\")\n",
        "else:\n",
        "    print(\"   ‚ùå ERROR: Algunos registros no fueron eliminados\")\n",
        "\n",
        "# Validaci√≥n 3: Verificar emails actualizados\n",
        "print(\"\\n‚úÖ Validaci√≥n 3: Verificar nuevos emails\")\n",
        "new_emails = final_state.filter(\"email LIKE '%_new@%'\")\n",
        "print(f\"   Emails con '_new@': {new_emails.count()} (esperado: 100)\")\n",
        "\n",
        "# Resumen\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä RESUMEN DE VALIDACI√ìN\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total final de registros: {final_state.count():,}\")\n",
        "print(f\"Registros actualizados: {updated_records.count()}\")\n",
        "print(f\"Registros eliminados: 10\")\n",
        "print(f\"Resultado: {'‚úÖ EXITOSO' if final_state.count() == 990 else '‚ùå ERROR'}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 7: Time Travel - Ver Historia de Cambios\n",
        "\n",
        "Delta Lake permite ver versiones anteriores de la tabla."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ver historia de versiones\n",
        "print(\"üïê Historia de versiones de la tabla:\")\n",
        "history = DeltaTable.forPath(spark, \"/tmp/delta/customers\").history()\n",
        "history.select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(truncate=False)\n",
        "\n",
        "# Leer versi√≥n anterior (antes del MERGE)\n",
        "print(\"\\nüìä Versi√≥n 0 (Initial Load):\")\n",
        "version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/delta/customers\")\n",
        "print(f\"   Registros: {version_0.count():,}\")\n",
        "\n",
        "print(\"\\nüìä Versi√≥n 1 (Despu√©s de MERGE):\")\n",
        "version_1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/tmp/delta/customers\")\n",
        "print(f\"   Registros: {version_1.count():,}\")\n",
        "\n",
        "print(\"\\n‚úÖ Time Travel permite auditor√≠a completa de cambios\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Conclusiones\n",
        "\n",
        "### ‚úÖ Lo que Aprendimos\n",
        "\n",
        "1. **Generar datos CDC** simulando Oracle GoldenGate\n",
        "2. **Deduplicaci√≥n** usando Window Functions\n",
        "3. **MERGE operation** en Delta Lake:\n",
        "   - UPDATE cuando hay match y es 'U'\n",
        "   - DELETE cuando hay match y es 'D'\n",
        "   - INSERT cuando no hay match\n",
        "4. **Validaci√≥n** de resultados\n",
        "5. **Time Travel** para auditor√≠a\n",
        "\n",
        "### üí° Best Practices\n",
        "\n",
        "- ‚úÖ **Siempre deduplicar** datos CDC antes del MERGE\n",
        "- ‚úÖ **Manejar DELETEs** expl√≠citamente\n",
        "- ‚úÖ **Usar Time Travel** para validaci√≥n\n",
        "- ‚úÖ **Idempotencia**: Pipeline debe ser re-ejecutable\n",
        "- ‚úÖ **Particionar datos CDC** por fecha para eficiencia\n",
        "\n",
        "### üîó Pr√≥ximos Pasos\n",
        "\n",
        "- Ver **Caso 14**: Data Quality & Error Handling\n",
        "- Ver **Caso 15**: Spark Structured Streaming (CDC real-time)\n",
        "- Implementar en producci√≥n con Airflow (Caso 16)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
